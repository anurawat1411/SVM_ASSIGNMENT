{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM AND NAIVE BAYES ASSIGNMENT"
      ],
      "metadata": {
        "id": "KSAe6eoIOF-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is a Support Vector Machine (SVM), and how does it work?"
      ],
      "metadata": {
        "id": "BcTqCIyzOOi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks, though it is mainly applied in classification problems. The main goal of SVM is to find the best decision boundary (hyperplane) that separates data points of different classes with the maximum margin.\n",
        "\n",
        "SVM works by plotting the data in an n-dimensional space (where n is the number of features) and identifying the hyperplane that distinctly divides the classes. The data points that are closest to the hyperplane are called support vectors, and they are the key elements in defining the position and orientation of the hyperplane.\n",
        "\n",
        "If the data is not linearly separable, SVM uses a technique called the kernel trick, which transforms the data into a higher-dimensional space where a separating hyperplane can be found. Common kernels include linear, polynomial, and radial basis function (RBF).\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose we want to classify emails as spam or not spam.\n",
        "SVM will find a boundary (hyperplane) that separates spam emails from non-spam ones based on their features (like word frequency, sender, etc.) with the largest possible margin."
      ],
      "metadata": {
        "id": "Fadk6aRdOT2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n"
      ],
      "metadata": {
        "id": "9VygZEuKOcvf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Support Vector Machines (SVM), the concept of margin refers to the distance between the separating hyperplane and the nearest data points (support vectors).\n",
        "Depending on how strictly the SVM separates the classes, there are two types — Hard Margin and Soft Margin SVM.\n",
        "\n",
        "1. Hard Margin SVM\n",
        "\n",
        "Used when the data is perfectly linearly separable.\n",
        "\n",
        "The algorithm finds a hyperplane that strictly separates the data into classes without any misclassification.\n",
        "\n",
        "It ensures a maximum margin between classes, but it is very sensitive to noise and outliers.\n",
        "\n",
        "If even one data point is misclassified, the model fails.\n",
        "\n",
        "Example:\n",
        "\n",
        " Ideal for clean, noise-free datasets where classes are clearly separable.\n",
        "\n",
        "2. Soft Margin SVM\n",
        "\n",
        "Used when data is not perfectly separable.\n",
        "\n",
        "It allows some misclassifications to achieve better generalization and performance on unseen data.\n",
        "\n",
        "Introduces a penalty parameter (C) that controls the trade-off between maximizing the margin and minimizing classification error.\n",
        "\n",
        "More robust and practical for real-world datasets containing noise or overlapping classes.\n",
        "\n",
        " Example:\n",
        "\n",
        "  Works well on datasets where classes overlap slightly or contain outliers."
      ],
      "metadata": {
        "id": "vTv5lcFQO4Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case."
      ],
      "metadata": {
        "id": "49E68Z7UPKrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Kernel Trick in Support Vector Machine (SVM) is a mathematical technique used to handle non-linear data.\n",
        "It allows SVM to transform the input data into a higher-dimensional space where it becomes easier to separate the classes using a linear boundary.\n",
        "\n",
        "Instead of explicitly calculating the coordinates of the data in that higher-dimensional space, the kernel trick computes the inner products between the transformed data points directly — saving time and computation.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "In real-world problems, data is often not linearly separable in its original form.\n",
        "\n",
        "The kernel function maps this data into a higher dimension where a linear separator (hyperplane) can be found.\n",
        "\n",
        "Example:\n",
        "\n",
        "Kernel: Radial Basis Function (RBF) Kernel (also known as Gaussian Kernel)\n",
        "\n",
        "Formula:\n",
        "\n",
        "K(x,x′)=exp(−γ∣∣x−x′∣∣2)\n",
        "\n",
        "Use Case:\n",
        "\n",
        "The RBF kernel is commonly used when data shows non-linear relationships.\n",
        "For example, in image classification or handwriting recognition, where patterns are curved or complex, RBF helps SVM create flexible decision boundaries to classify data accurately."
      ],
      "metadata": {
        "id": "GpqIlRL4PU41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?"
      ],
      "metadata": {
        "id": "yinjBj-TP-Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Naïve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem, used for classification tasks.\n",
        "It predicts the probability that a data point belongs to a particular class based on prior knowledge and the likelihood of features.\n",
        "\n",
        "Bayes’ Theorem:\n",
        "\n",
        "P(C∣X)=P(X)P(X∣C)⋅P(C)​\n",
        "\n",
        "Where:\n",
        "\n",
        "P(C∣X):Posterior probability (probability of class given features)\n",
        "\n",
        "P(X∣C):Likelihood (probability of features given class)\n",
        "\n",
        "P(C):Prior probability of the class\n",
        "\n",
        "P(X):Probability of the feature\n",
        "\n",
        ">>Why it is called “Naïve”:\n",
        "\n",
        "\n",
        "It is called “naïve” because it assumes that all features are independent of each other — meaning one feature’s presence does not affect another.\n",
        "In reality, this assumption is rarely true, but it still performs surprisingly well in many practical cases.\n",
        "\n",
        ">Example Use Case:\n",
        "\n",
        "Email Spam Detection:\n",
        "Each word in an email is treated as an independent feature.\n",
        "The model predicts whether the email is spam or not spam based on the probability of words appearing in each category."
      ],
      "metadata": {
        "id": "SXHoOFb2QCla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n"
      ],
      "metadata": {
        "id": "YbdRtA-XQ4os"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naïve Bayes Classifier has different variants depending on the type of data it works with.\n",
        "The three most common types are Gaussian, Multinomial, and Bernoulli Naïve Bayes.\n",
        "\n",
        "1. Gaussian Naïve Bayes\n",
        "\n",
        "Used when features are continuous (numerical) and follow a normal (Gaussian) distribution.\n",
        "\n",
        "It assumes that the continuous values associated with each class are distributed according to a Gaussian (bell-shaped) curve.\n",
        "\n",
        ">Use Case:\n",
        "\n",
        "Suitable for datasets with real-valued features such as height, weight, age, or temperature.\n",
        "\n",
        "Example: Iris flower classification, where petal length and width are continuous values.\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "Used when features represent discrete counts or frequencies.\n",
        "\n",
        "It assumes that the features follow a Multinomial distribution (common in text data).\n",
        "\n",
        ">Use Case:\n",
        "\n",
        "Works best for text classification problems, such as spam detection or document categorization, where features are word counts or term frequencies.\n",
        "\n",
        "Example: Email spam classification based on word occurrences.\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "Used when features are binary (0 or 1) — representing the presence or absence of a feature.\n",
        "\n",
        "It models data as a series of yes/no features rather than counts.\n",
        "\n",
        ">Use Case:\n",
        "\n",
        "Works well for binary text classification, where only the presence or absence of words matters, not their frequency.\n",
        "\n",
        "Example: Sentiment analysis — checking if certain positive or negative words exist in a review."
      ],
      "metadata": {
        "id": "wPqTkA4qR7v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "\n",
        "● Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "vO-I25nTSMjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d02bs-WMN70t",
        "outputId": "61e26c50-c88d-48be-d9a2-ee0c52395b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_model = SVC(kernel='linear')\n",
        "\n",
        "\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "HgxAPZJFsDQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYU7ao-ksPMH",
        "outputId": "cda3f8a5-574a-4449-b040-603e88ad8e72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        43\n",
            "           1       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy.\n"
      ],
      "metadata": {
        "id": "qPegLlQtsYwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the SVM model\n",
        "svm = SVC()\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(svm, param_grid, refit=True, verbose=0)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqqCMukbshuT",
        "outputId": "3a325a02-0d14-4f79-8c15-424c8293761d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "a8gqtoWqs4L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "data = fetch_20newsgroups(subset='all', categories=['rec.sport.baseball', 'sci.med'], remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_prob = nb.predict_proba(X_test)\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1])\n",
        "roc_auc = roc_auc_score(y_test_bin, y_prob[:, 1])\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YWf3Yo2tHKe",
        "outputId": "7af319df-3445-48a4-f7aa-0370476c4df3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9971052765222691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Imagine you’re working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution."
      ],
      "metadata": {
        "id": "eMKMGDC0tvxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preprocessing\n",
        "\n",
        "To classify emails as Spam or Not Spam, data preprocessing is the most important step. The main tasks include:\n",
        "\n",
        ">>Handling Missing Data\n",
        "\n",
        "Emails may have missing text or metadata (like subject line or sender).\n",
        "\n",
        "Replace missing values with an empty string (\"\") or use imputation techniques for numerical features (if any).\n",
        "\n",
        "Remove irrelevant fields such as message IDs or timestamps if they don’t contribute to the classification.\n",
        "\n",
        ">>Text Cleaning and Tokenization\n",
        "\n",
        "Convert all text to lowercase.\n",
        "\n",
        "Remove punctuation, numbers, and special characters.\n",
        "\n",
        "Remove stopwords (like “is”, “the”, “and”) using NLTK or SpaCy.\n",
        "\n",
        "Apply stemming or lemmatization to reduce words to their root form (e.g., “running” → “run”).\n",
        "\n",
        ">>Text Vectorization\n",
        "\n",
        "Convert text into numerical format using:\n",
        "\n",
        "TF-IDF (Term Frequency–Inverse Document Frequency) – gives importance to rare but meaningful words.\n",
        "\n",
        "Or Count Vectorizer – simple word frequency representation.\n",
        "\n",
        "TF-IDF is preferred for spam detection because it reduces the weight of common words like “email”, “please”, etc.\n",
        "\n",
        "2. Model Selection and Justification\n",
        "\n",
        ">>Naïve Bayes Classifier\n",
        "\n",
        "Works exceptionally well for text classification tasks because it assumes feature independence, which fits natural language data fairly well.\n",
        "\n",
        "Multinomial Naïve Bayes performs well with word counts or TF-IDF values.\n",
        "\n",
        "Fast, efficient, and performs well even on smaller datasets.\n",
        "\n",
        ">>SVM (Support Vector Machine)\n",
        "\n",
        "SVM can be used if the dataset is large and complex, as it handles high-dimensional data efficiently.\n",
        "\n",
        "It works well for separating non-linear data using kernel functions, but it’s slower for very large text datasets.\n",
        "\n",
        "Choice:\n",
        "\n",
        "For this task, Multinomial Naïve Bayes is preferred due to its simplicity, speed, and effectiveness for high-dimensional sparse text data.\n",
        "\n",
        "3. Addressing Class Imbalance\n",
        "\n",
        "If there are more legitimate emails than spam, we can handle imbalance by:\n",
        "\n",
        "Resampling Techniques:\n",
        "\n",
        "Oversampling minority class (e.g., using SMOTE)\n",
        "\n",
        "Undersampling majority class\n",
        "\n",
        "Class weights: Adjust model parameters to give more importance to the minority (spam) class.\n",
        "\n",
        "Threshold tuning: Adjust decision thresholds to reduce false negatives (missed spam emails).\n",
        "\n",
        "4. Model Evaluation Metrics\n",
        "\n",
        "To evaluate performance, use metrics suitable for imbalanced data:\n",
        "\n",
        "Metric\tPurpose\n",
        "Accuracy\tBasic performance indicator, but can be misleading if classes are imbalanced.\n",
        "Precision\tMeasures how many emails predicted as spam are actually spam.\n",
        "Recall (Sensitivity)\tMeasures how many actual spam emails were correctly identified.\n",
        "F1-Score\tHarmonic mean of Precision and Recall — good for imbalanced datasets.\n",
        "ROC-AUC Score\tMeasures overall model discrimination between spam and not spam.\n",
        "\n",
        ">Preferred metrics: Precision, Recall, F1-Score, and ROC-AUC (not just accuracy).\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "Implementing this spam classification system can have a significant positive impact:\n",
        "\n",
        ">Improves productivity: Employees spend less time sorting or deleting spam emails.\n",
        "\n",
        ">Enhances cybersecurity: Reduces phishing and malware risks.\n",
        "\n",
        ">Increases efficiency: Ensures legitimate emails are prioritized, improving communication flow.\n",
        "\n",
        ">Saves costs: Less risk of data breaches or wasted resources due to spam attacks."
      ],
      "metadata": {
        "id": "bi6ebQEcutET"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1DQCwyfLvPYI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}